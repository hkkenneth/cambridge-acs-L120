\documentclass{article}

\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}


\title{Data Analysis and Visualisation using R}
\date{2016-01-11}
\author{\textbf{Wing Cheung Kenneth Lui} \\
Computer Laboratory, University of Cambridge \\
{\tt wckl2@cam.ac.uk}}

\begin{document}
  \pagenumbering{arabic}
  \maketitle

\section{Introduction}

In this study, we analyse a training dataset provided in CSV (comma-separated values) format. There are 1800 observations (i.e. rows) and 12 columns of values in total. The first column, named {\tt id}, contains unique integer identifiers from 1 to 1800; the second column, named {\tt Y}, stores a number without containing any {\tt NA} values; {\tt Y} is followed by nine floating point numeric columns named from {\tt X1}, {\tt X2},  \(...\), {\tt X9}, which may contain {\tt NA}s in some observations; finally, the last column, named {\tt label}, contains one of eight text values such as ash, beech, oak and so on.

We divide the data into eight uneven subsets based on the label value. During preprocessing, {\tt id}, {\tt label} and {\tt NA} entries are removed from each subset. The number of observations and the remaining column names are tabulated in Table~\ref{subset-table}. In statistical learning context, the training data are observations where the response variable Y may have some relationship with one or more of the potential predictor variables {\tt X\textsubscript{i}}.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline \bf Label & \bf Number of observations & \bf Predictor variables \\ \hline
ash & 500 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
beech & 500 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
elder & 500 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
elm & 100 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
larch & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
oak & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
rowan & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
yew & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
\end{tabular}
\end{center}
\caption{\label{subset-table} Overview of subset in training data.}
\end{table}

\section{Statistical Analyses}

A variety of regression and classification techniques have been employed to detect characteristics and dependencies underlying the training data. The results are presented graphically and numerically in this report.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
\hline \bf Library & \bf Functions provided \\ \hline
{\textit leaps} & {\tt regsubsets()} \\ \hline
\end{tabular}
\end{center}
\caption{\label{library-table} Overview of libraries used.}
\end{table}

\subsection{Ash subset}

The response variable {\tt Y} is continuous so regression methods are used. First, we use {\tt pairs()} to produce a matrix of pairwise scatterplots for each pair of variables ({\tt Y} and all {\tt X\textsubscript{i}}) to detect whether any observable patterns exist. {\tt X7} shows an obvious linear relationship with {\tt Y} in these plots so we use {\tt lm()} to fit a simple linear regression between them. The summary of this fit shows that the least squares coefficients of {\tt X7} in the linear relationship is highly significant (p-value is close to zero). Its training $R^2$ value of 0.9966 shows that the variance left unexplained by the linear fit is negligible. This fit also results in a convincing inspection plot as shown in Figure~\ref{fig:01-ash}. No obvious pattern can seen in the residuals vs fitted values plot. However, this should not rule out the possibility that {\tt Y} is also related to some other predictors. we perform a multiple linear regression using all available predictors and obtain an ``improved'' fit where both {\tt X3} and {\tt X7} are significant and training $R^2$ value is 0.9988. But such improvement must be taken with a grain of salt because training $R^2$ values for any more flexible models (i.e. those with more predictors) must be no less than a more restrictive one. To verify whether a simple linear model involving {\tt X7} only is the true relationship over other more flexible multiple linear models, 5-fold cross-validation has been used with best subset variables selection. As shown in Listing~\ref{ash}, cross-validation suggests $Y \sim X3+X4+X7+X8$ as the model achieving the lowest test squared error but the performance difference compared to $Y \sim X3$ is small, which is chosen for the sake of interpretability.

\begin{figure}[h!]
  \includegraphics[width=\linewidth]{project/images/01-ash.png}
  \caption{Inspection plots for linear fit $Y \sim X7$ in ash dataset.}
  \label{fig:01-ash}
\end{figure}

\newpage

\paragraph{Appendices}

\lstinputlisting[breaklines=true,language=R,caption={sharecode.R},label=sharedcode]{project/1-sharedcode.R}

\rule{\textwidth}{1pt}

\lstinputlisting[breaklines=true,language=R,caption={preprocessing.R},label=preprocessing]{project/2-preprocessing.R}

\rule{\textwidth}{1pt}

\newpage

\lstinputlisting[breaklines=true,language=R,caption={ash.R},label=ash]{project/3-ash.R}

\rule{\textwidth}{1pt}


\end{document}