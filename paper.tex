\documentclass{article}

\usepackage{amsmath}
\usepackage{multicol}

\title{Data Analysis and Visualisation using R}
\date{2016-01-11}
\author{\textbf{Wing Cheung Kenneth Lui} \\
Computer Laboratory, University of Cambridge \\
{\tt wckl2@cam.ac.uk}}

\begin{document}
  \pagenumbering{arabic}
  \maketitle

\section{Introduction}

In this study, we analyse a training dataset provided in CSV (comma-separated values) format. There are 1800 observations (i.e. rows) and 12 columns of values in total. The first column, named {\tt id}, contains unique integer identifiers from 1 to 1800; the second column, named {\tt Y}, stores a number without containing any {\tt NA} values; {\tt Y} is followed by nine floating point numeric columns named from {\tt X1}, {\tt X2},  \(...\), {\tt X9}, which may contain {\tt NA}s in some observations; finally, the last column, named {\tt label}, contains one of eight text values such as ash, beech, oak and so on.

We divide the data into eight uneven subsets based on the label value. During preprocessing, {\tt id}, {\tt label} and {\tt NA} entries are removed from each subset. The number of observations and the remaining column names are tabulated in Table~\ref{subset-table}. In statistical learning context, the training data are observations where the response variable Y may have some relationship with one or more of the potential predictor variables {\tt X\textsubscript{i}}.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline \bf Label & \bf Number of observations & \bf Predictor variables \\ \hline
ash & 500 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
beech & 500 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
elder & 500 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
elm & 100 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
larch & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
oak & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
rowan & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
yew & 50 & {\tt X1, X2, X3, X4, X5, X6, X7, X8, X9} \\ \hline
\end{tabular}
\end{center}
\caption{\label{subset-table} Overview of subset in training data.}
\end{table}

\section{Statistical Analyses}

A variety of regression and classification techniques have been employed to detect characteristics and dependencies underlying the training data. The results are presented graphically and numerically in this report.

\subsection{Ash subset}

The response variable {\tt Y} is continuous so I analyse this subset using regression methods. First, I use {\tt pairs()} to produce a matrix of pairwise scatterplots for each pair of variables ({\tt Y} and all {\tt X\textsubscript{i}}) to detect whether there are any observable patterns. {\tt X7} shows an obvious linear relationship with {\tt Y} in these plots so I use {\tt lm()} to fit a simple linear regression between them. The summary of this fit shows that the least squares coefficients of {\tt X7} in the linear relationship is highly significant (p-value is close to zero) and training $R^2$ value of 0.9966 is a measure showing that variance left unexplained by the linear fit is negligible. However, this should not rule out the possibility that {\tt Y} is also related to some of the other eight predictors. I perform a multiple linear regression using all available predictors and obtain an ``improved'' fit where both {\tt X3} and {\tt X7} are significant and training $R^2$ value is 0.9988. But such improvement must be taken with a grain of salt because training $R^2$ values for any more flexible models (i.e. those with more predictors) must be no less than a more restrictive one. To find out whether a simple linear model involving {\tt X7} only is the true relationship over other more flexible multiple linear models, k?-fold cross validation has been used.  X3, X4, X7

\paragraph{Dataset}

Some more text.

This formula $f(x) = x^2$ is an example.

\subparagraph{Subparagraph}

Even more text.

\begin{equation*}
  f(x) = x^2
\end{equation*}

\begin{align*}
  1 + 2 &= 3\\
  1 &= 3 - 2
\end{align*}

\end{document}